{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1BOV_Q3NMdwPohQLAFUyfBxLUl671tS5V",
      "authorship_tag": "ABX9TyOzzw4elfA+dJz+4Qo+CZHP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axelpuyo/liasd/blob/master/perceptive_bias_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs\n",
        "Installs requirements and mounts file to Google Drive in order to save models."
      ],
      "metadata": {
        "id": "-F3tF7apvOy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Run once\n",
        "!pip -q install import-ipynb\n",
        "!pip -q install tensorflow==2.8\n",
        "!apt -q install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# check path\n",
        "import os\n",
        "print(os.getcwd())\n",
        "print(os.listdir())\n",
        "\n",
        "# cd your_path\n",
        "%cd drive/MyDrive/Colab Notebooks/liasd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feOUvwgQCRRm",
        "outputId": "b79ee06f-c861-47d8-81d8-563ed6de3bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "libcudnn8 is already the newest version (8.1.0.77-1+cuda11.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 10 not upgraded.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Colab Notebooks/liasd\n",
            "['perceptive_bias_model.ipynb', 'saved_models', 'explainers.ipynb', 'post_analysis.ipynb', 'utils.ipynb', 'colored_mnist.ipynb']\n",
            "[Errno 2] No such file or directory: 'drive/MyDrive/Colab Notebooks/liasd'\n",
            "/content/drive/MyDrive/Colab Notebooks/liasd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n",
        "Necessary imports, please note that when any imported .ipynb files are modified, you need to restart runtime (CTRL + M or Runtime > Restart Runtime) in order for changes to take effect."
      ],
      "metadata": {
        "id": "Yj5SPA-Dvcqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Imports\n",
        "import import_ipynb\n",
        "import utils\n",
        "import colored_mnist\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OQHN8yUonbU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP's `DeepExplainer` does not support TensorFlow 2.0+ (namely eager-mode by default), while `Grad-CAM` requires it. Thus, we train the same model on two different versions of TensorFlow (1.x and 2.x behavior). However, the behavior of TensorFlow can only be changed at the beginning of a runtime. **In order to switch versions, you need to restart runtime**. \n",
        "\n"
      ],
      "metadata": {
        "id": "GLtkdkjvvp7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## TensorFlow version swapping (SHAP works with TF 1.x, GRAD-CAM with TF 2.x) \n",
        "tf1 = False\n",
        "if tf1:\n",
        "  tf.compat.v1.disable_v2_behavior()\n",
        "  tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "print('TensorFlow 2.x behavior ? ', tf.executing_eagerly())"
      ],
      "metadata": {
        "id": "Iao542UeXa08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10772dd2-7e65-4737-d317-1df452bf1fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 2.x behavior ?  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters and load dataset"
      ],
      "metadata": {
        "id": "O6wJNA6r30xZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Parameters\n",
        "num = 8000 # number of images to pull from original MNIST\n",
        "split = (.7, .2, .1) # train/val/test split : (%train, %validation, %test) in 0 - 1 fraction\n",
        "level = 0 # noise level (0-1)\n",
        "\n",
        "## Fetch Colored-MNIST\n",
        "# Colored-MNIST v.1. through v.3. : warning, v.3. is heavily biased (see report)\n",
        "# bools = (False, False) # bools = (False, False) --> colored digits on black background (v1) ; bools = (True, False) --> colored digits, colored background (v2); bools = (True, True) --> colored digits, colored background, colored noise (v3. warning, this dataset is biased)\n",
        "# (train_ims, test_ims, val_ims), (train_color, test_color, val_color), (train_shape, test_shape, val_shape), (train_rule, test_rule, val_rule) = colored_mnist.load_data(num, mnist_type='old', noise_level=level, bools=bools)\n",
        "\n",
        "# Colored-MNIST v.4.\n",
        "(train_ims, test_ims, val_ims), (train_color, test_color, val_color), (train_shape, test_shape, val_shape), (train_rule, test_rule, val_rule) = colored_mnist.load_data(num, noise_level=level, split=split)"
      ],
      "metadata": {
        "id": "FNBUkfcGngb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "198d372a-03e1-42a7-d1bd-c71469643bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> checking dataset regularity\n",
            "label r / counts 421\n",
            "label 9 / counts 514\n",
            "label 8 / counts 478\n",
            "label 7 / counts 579\n",
            "label 6 / counts 512\n",
            "label 5 / counts 443\n",
            "label 4 / counts 555\n",
            "label 3 / counts 520\n",
            "label 2 / counts 488\n",
            "label 1 / counts 575\n",
            "label 0 / counts 515\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model architecture and training"
      ],
      "metadata": {
        "id": "_m40YMTr38sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Models\n",
        "# Color-biased model\n",
        "color_model = keras.Sequential()\n",
        "color_model.add(keras.layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu', input_shape=train_ims[0].shape, name='conv1'))\n",
        "color_model.add(keras.layers.MaxPool2D(pool_size=(2,2), name='pool1'))\n",
        "color_model.add(keras.layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu', name='conv2'))\n",
        "color_model.add(keras.layers.MaxPool2D(pool_size=(2,2), padding='same', name='pool2'))\n",
        "color_model.add(keras.layers.Flatten())\n",
        "color_model.add(keras.layers.Dense(13, activation=tf.keras.activations.softmax))\n",
        "\n",
        "input_shape = train_ims.shape\n",
        "color_model.build(input_shape)\n",
        "\n",
        "color_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# color_model.summary()\n",
        "\n",
        "print('>> Color-biased model training')\n",
        "color_model.fit(train_ims, train_color, batch_size=12, epochs=5, shuffle=True)"
      ],
      "metadata": {
        "id": "PSKd4PNJulZc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d502f09-d383-4aba-f0d6-50bbc3b5ffdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Color-biased model training\n",
            "Epoch 1/5\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0257 - accuracy: 0.9959\n",
            "Epoch 2/5\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 3.8956e-09 - accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "467/467 [==============================] - 3s 6ms/step - loss: 1.0005e-09 - accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "467/467 [==============================] - 3s 6ms/step - loss: 5.3218e-10 - accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "467/467 [==============================] - 3s 6ms/step - loss: 3.1931e-10 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1e02577bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3x7CduVnQt0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c246803b-fde0-4f30-c2ba-72bd4e591525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Shape-biased model training\n",
            "Epoch 1/15\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.5124 - accuracy: 0.8427\n",
            "Epoch 2/15\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.1722 - accuracy: 0.9523\n",
            "Epoch 3/15\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.1196 - accuracy: 0.9666\n",
            "Epoch 4/15\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0874 - accuracy: 0.9755\n",
            "Epoch 5/15\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0684 - accuracy: 0.9804\n",
            "Epoch 6/15\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0578 - accuracy: 0.9850\n",
            "Epoch 7/15\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0470 - accuracy: 0.9857\n",
            "Epoch 8/15\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0387 - accuracy: 0.9880\n",
            "Epoch 9/15\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0315 - accuracy: 0.9920\n",
            "Epoch 10/15\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0277 - accuracy: 0.9921\n",
            "Epoch 11/15\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0208 - accuracy: 0.9937\n",
            "Epoch 12/15\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0176 - accuracy: 0.9955\n",
            "Epoch 13/15\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0160 - accuracy: 0.9973\n",
            "Epoch 14/15\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0123 - accuracy: 0.9973\n",
            "Epoch 15/15\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0098 - accuracy: 0.9966\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1dd00a2210>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Shape-biased model\n",
        "shape_model = keras.Sequential()\n",
        "shape_model.add(keras.layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu', input_shape=train_ims[0].shape, name='conv1'))\n",
        "shape_model.add(keras.layers.MaxPool2D(pool_size=(2,2), name='pool1'))\n",
        "shape_model.add(keras.layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu', name='conv2'))\n",
        "shape_model.add(keras.layers.MaxPool2D(pool_size=(2,2), padding='same', name='pool2'))\n",
        "shape_model.add(keras.layers.Flatten())\n",
        "shape_model.add(keras.layers.Dense(13, activation=tf.keras.activations.softmax))\n",
        "\n",
        "input_shape = train_ims.shape\n",
        "shape_model.build(input_shape)\n",
        "\n",
        "shape_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# shape_model.summary()\n",
        "\n",
        "print('>> Shape-biased model training')\n",
        "shape_model.fit(train_ims, train_shape, batch_size=12, epochs=15, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rule-biased model\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu', input_shape=train_ims[0].shape, name='conv1'))\n",
        "model.add(keras.layers.MaxPool2D(pool_size=(2,2), name='pool1'))\n",
        "model.add(keras.layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu', name='conv2'))\n",
        "model.add(keras.layers.MaxPool2D(pool_size=(2,2), padding='same', name='pool2'))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(13, activation=tf.keras.activations.softmax))\n",
        "\n",
        "input_shape = train_ims.shape\n",
        "model.build(input_shape)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# model.summary()\n",
        "\n",
        "print('>> Rule-biased model training')\n",
        "model.fit(train_ims, train_rule, batch_size=12, epochs=25, shuffle=True)"
      ],
      "metadata": {
        "id": "v-JrxVlruuu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60cc61a2-03bd-4f06-b242-54f3972bafd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Rule-biased model training\n",
            "Epoch 1/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.4984 - accuracy: 0.8462\n",
            "Epoch 2/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.1728 - accuracy: 0.9514\n",
            "Epoch 3/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.1173 - accuracy: 0.9698\n",
            "Epoch 4/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0831 - accuracy: 0.9732\n",
            "Epoch 5/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0720 - accuracy: 0.9793\n",
            "Epoch 6/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0538 - accuracy: 0.9846\n",
            "Epoch 7/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0455 - accuracy: 0.9871\n",
            "Epoch 8/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0387 - accuracy: 0.9911\n",
            "Epoch 9/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0328 - accuracy: 0.9904\n",
            "Epoch 10/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0273 - accuracy: 0.9932\n",
            "Epoch 11/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0245 - accuracy: 0.9934\n",
            "Epoch 12/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0172 - accuracy: 0.9955\n",
            "Epoch 13/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0152 - accuracy: 0.9962\n",
            "Epoch 14/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0121 - accuracy: 0.9968\n",
            "Epoch 15/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0110 - accuracy: 0.9971\n",
            "Epoch 16/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0088 - accuracy: 0.9979\n",
            "Epoch 17/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0046 - accuracy: 0.9989\n",
            "Epoch 18/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0056 - accuracy: 0.9986\n",
            "Epoch 19/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0045 - accuracy: 0.9989\n",
            "Epoch 20/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0040 - accuracy: 0.9989\n",
            "Epoch 21/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0018 - accuracy: 0.9996\n",
            "Epoch 22/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0029 - accuracy: 0.9989\n",
            "Epoch 23/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 7.8397e-04 - accuracy: 0.9998\n",
            "Epoch 24/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0010 - accuracy: 0.9996\n",
            "Epoch 25/25\n",
            "467/467 [==============================] - 2s 4ms/step - loss: 0.0011 - accuracy: 0.9996\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1d9458ca90>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rule-biased model 2 : VGG-16 architecture\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "\n",
        "input_shape = train_ims[0].shape\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=input_shape,filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2),padding=\"same\"))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2),padding=\"same\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2),padding=\"same\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='conv2'))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2),padding=\"same\"))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=4096,activation=\"relu\"))\n",
        "model.add(Dense(units=4096,activation=\"relu\"))\n",
        "model.add(Dense(units=13, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# model.summary()\n",
        "\n",
        "print('>> VGG16 Rule-biased model training')\n",
        "model.fit(train_ims, train_rule, batch_size=12, epochs=25, shuffle=True)"
      ],
      "metadata": {
        "id": "IHuhSMtGqTPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8c5913-0358-405f-c097-b3c36b51085d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> VGG16 Rule-biased model training\n",
            "Epoch 1/25\n",
            "467/467 [==============================] - 17s 33ms/step - loss: 2.9682 - accuracy: 0.1037\n",
            "Epoch 2/25\n",
            "467/467 [==============================] - 16s 33ms/step - loss: 2.3978 - accuracy: 0.0962\n",
            "Epoch 3/25\n",
            "467/467 [==============================] - 16s 33ms/step - loss: 2.3964 - accuracy: 0.0968\n",
            "Epoch 4/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3959 - accuracy: 0.0945\n",
            "Epoch 5/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3954 - accuracy: 0.1023\n",
            "Epoch 6/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3951 - accuracy: 0.1009\n",
            "Epoch 7/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3950 - accuracy: 0.1014\n",
            "Epoch 8/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3948 - accuracy: 0.1048\n",
            "Epoch 9/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3948 - accuracy: 0.1007\n",
            "Epoch 10/25\n",
            "467/467 [==============================] - 16s 33ms/step - loss: 2.3947 - accuracy: 0.1014\n",
            "Epoch 11/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3946 - accuracy: 0.0980\n",
            "Epoch 12/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3946 - accuracy: 0.1041\n",
            "Epoch 13/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3945 - accuracy: 0.1025\n",
            "Epoch 14/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3944 - accuracy: 0.1045\n",
            "Epoch 15/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3942 - accuracy: 0.0938\n",
            "Epoch 16/25\n",
            "467/467 [==============================] - 16s 33ms/step - loss: 2.3943 - accuracy: 0.0950\n",
            "Epoch 17/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3942 - accuracy: 0.0998\n",
            "Epoch 18/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3942 - accuracy: 0.1021\n",
            "Epoch 19/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3942 - accuracy: 0.1005\n",
            "Epoch 20/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3941 - accuracy: 0.0950\n",
            "Epoch 21/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3941 - accuracy: 0.1011\n",
            "Epoch 22/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3941 - accuracy: 0.1007\n",
            "Epoch 23/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3940 - accuracy: 0.1018\n",
            "Epoch 24/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3939 - accuracy: 0.1005\n",
            "Epoch 25/25\n",
            "467/467 [==============================] - 16s 34ms/step - loss: 2.3940 - accuracy: 0.1016\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1d9436f810>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save\n",
        "Saves models in tf1 or tf2 directories depending on the TensorFlow version that was used."
      ],
      "metadata": {
        "id": "tBWaer8Fww4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Save models\n",
        "# get corresponding path\n",
        "color_path = utils.pathfinder(tf1, level, 'color')\n",
        "shape_path = utils.pathfinder(tf1, level, 'shape')\n",
        "rule_path = utils.pathfinder(tf1, level, 'rule')\n",
        "\n",
        "# save to path and overwrite if necessary\n",
        "color_model.save(color_path, overwrite=True)\n",
        "shape_model.save(shape_path, overwrite=True)\n",
        "model.save(rule_path, overwrite=True)"
      ],
      "metadata": {
        "id": "hzV4kV9gmLVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rule_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aImrQKvBOS4V",
        "outputId": "33fe9f98-ccbb-4549-cd0b-6704cbdafe6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/liasd/saved_models/tf2/rule_biased0\n"
          ]
        }
      ]
    }
  ]
}