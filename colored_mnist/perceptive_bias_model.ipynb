{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "perceptive_bias_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1BOV_Q3NMdwPohQLAFUyfBxLUl671tS5V",
      "authorship_tag": "ABX9TyP3Fk/Cw+JrxRZEAjkoeh55",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axelpuyo/liasd/blob/master/perceptive_bias_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Run once\n",
        "!pip install import-ipynb\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "print(os.getcwd())\n",
        "print(os.listdir())\n",
        "\n",
        "#%cd your_path/Colab Notebooks\n",
        "%cd drive/MyDrive/Colab Notebooks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feOUvwgQCRRm",
        "outputId": "c7f522f6-da7c-4928-ef7c-a5df3e73051d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: import-ipynb in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (5.4.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (5.5.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.4.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import-ipynb) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import-ipynb) (0.2.5)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.11.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (2.15.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (4.12.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (5.8.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (4.1.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (21.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->import-ipynb) (3.8.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->import-ipynb) (0.7.0)\n",
            "/content/drive/MyDrive/Colab Notebooks\n",
            "['Untitled0.ipynb', 'Data', 'feature_map_analysis', 'Untitled1.ipynb', '1. Image Collection.ipynb', '2. Training and Detection.ipynb', 'Untitled2.ipynb', 'Untitled', 'Copy of Image classification with Visual Attention Network (VAN)', '.ipynb_checkpoints', 'colored_mnist.ipynb', 'ongoing.ipynb']\n",
            "[Errno 2] No such file or directory: 'drive/MyDrive/Colab Notebooks'\n",
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random as rd\n",
        "import import_ipynb\n",
        "import colored_mnist\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OQHN8yUonbU7"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_colors, train_numbers, train_mixed), (test_images, test_colors, test_numbers, test_mixed) = colored_mnist.load_data(8000)"
      ],
      "metadata": {
        "id": "FNBUkfcGngb6"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Color-biased model\n",
        "color_model = keras.Sequential()\n",
        "color_model.add(keras.layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "color_model.add(keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "color_model.add(keras.layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "color_model.add(keras.layers.MaxPool2D(pool_size=(2,2), padding='same'))\n",
        "color_model.add(keras.layers.Flatten())\n",
        "color_model.add(keras.layers.Dense(3))\n",
        "color_model.add(keras.layers.Softmax())\n",
        "\n",
        "input_shape = train_images.shape\n",
        "color_model.build(input_shape)\n",
        "\n",
        "color_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# color_model.summary()\n",
        "\n",
        "print('>> Color-biased model training')\n",
        "color_model.fit(train_images, train_colors, batch_size=12, epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSKd4PNJulZc",
        "outputId": "f0d67388-c6ff-4597-a4c7-618ee962b475"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Color-biased model training\n",
            "Epoch 1/25\n",
            "667/667 [==============================] - 3s 4ms/step - loss: 0.0069 - accuracy: 0.9981\n",
            "Epoch 2/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 1.4454e-09 - accuracy: 1.0000\n",
            "Epoch 3/25\n",
            "667/667 [==============================] - 4s 6ms/step - loss: 2.9802e-10 - accuracy: 1.0000\n",
            "Epoch 4/25\n",
            "667/667 [==============================] - 4s 6ms/step - loss: 1.6391e-10 - accuracy: 1.0000\n",
            "Epoch 5/25\n",
            "667/667 [==============================] - 3s 5ms/step - loss: 1.0431e-10 - accuracy: 1.0000\n",
            "Epoch 6/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 1.0431e-10 - accuracy: 1.0000\n",
            "Epoch 7/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 7.4506e-11 - accuracy: 1.0000\n",
            "Epoch 8/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 5.9605e-11 - accuracy: 1.0000\n",
            "Epoch 9/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 4.4703e-11 - accuracy: 1.0000\n",
            "Epoch 10/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 5.9605e-11 - accuracy: 1.0000\n",
            "Epoch 11/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 2.9802e-11 - accuracy: 1.0000\n",
            "Epoch 12/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 2.9802e-11 - accuracy: 1.0000\n",
            "Epoch 13/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 4.4703e-11 - accuracy: 1.0000\n",
            "Epoch 14/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 2.9802e-11 - accuracy: 1.0000\n",
            "Epoch 15/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 16/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 17/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 18/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 19/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 20/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 21/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 22/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 23/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 24/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
            "Epoch 25/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5d0c256f90>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "R3x7CduVnQt0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41400fd2-8312-4a14-c424-110625a18faa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Shape-biased model training\n",
            "Epoch 1/25\n",
            "667/667 [==============================] - 3s 4ms/step - loss: 0.3814 - accuracy: 0.8846\n",
            "Epoch 2/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 0.1335 - accuracy: 0.9605\n",
            "Epoch 3/25\n",
            "667/667 [==============================] - 4s 6ms/step - loss: 0.0951 - accuracy: 0.9726\n",
            "Epoch 4/25\n",
            "667/667 [==============================] - 4s 6ms/step - loss: 0.0717 - accuracy: 0.9796\n",
            "Epoch 5/25\n",
            "667/667 [==============================] - 3s 5ms/step - loss: 0.0576 - accuracy: 0.9843\n",
            "Epoch 6/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 0.0529 - accuracy: 0.9860\n",
            "Epoch 7/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 0.0418 - accuracy: 0.9879\n",
            "Epoch 8/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 0.0325 - accuracy: 0.9912\n",
            "Epoch 9/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0275 - accuracy: 0.9923\n",
            "Epoch 10/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0229 - accuracy: 0.9939\n",
            "Epoch 11/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0203 - accuracy: 0.9949\n",
            "Epoch 12/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0164 - accuracy: 0.9952\n",
            "Epoch 13/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0143 - accuracy: 0.9966\n",
            "Epoch 14/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0093 - accuracy: 0.9967\n",
            "Epoch 15/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 0.0092 - accuracy: 0.9974\n",
            "Epoch 16/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 0.0070 - accuracy: 0.9975\n",
            "Epoch 17/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 0.0071 - accuracy: 0.9984\n",
            "Epoch 18/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 0.0025 - accuracy: 0.9994\n",
            "Epoch 19/25\n",
            "667/667 [==============================] - 3s 4ms/step - loss: 0.0025 - accuracy: 0.9991\n",
            "Epoch 20/25\n",
            "667/667 [==============================] - 3s 4ms/step - loss: 0.0021 - accuracy: 0.9992\n",
            "Epoch 21/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0025 - accuracy: 0.9994\n",
            "Epoch 22/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0042 - accuracy: 0.9990\n",
            "Epoch 23/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0021 - accuracy: 0.9994\n",
            "Epoch 24/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0017 - accuracy: 0.9992\n",
            "Epoch 25/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0011 - accuracy: 0.9994\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5d0c0af790>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# Shape-biased model\n",
        "shape_model = keras.Sequential()\n",
        "shape_model.add(keras.layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "shape_model.add(keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "shape_model.add(keras.layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "shape_model.add(keras.layers.MaxPool2D(pool_size=(2,2), padding='same'))\n",
        "shape_model.add(keras.layers.Flatten())\n",
        "shape_model.add(keras.layers.Dense(10))\n",
        "shape_model.add(keras.layers.Softmax())\n",
        "\n",
        "input_shape = train_images.shape\n",
        "shape_model.build(input_shape)\n",
        "\n",
        "shape_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# shape_model.summary()\n",
        "\n",
        "print('>> Shape-biased model training')\n",
        "shape_model.fit(train_images, train_numbers, batch_size=12, epochs=25, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rule-biased model\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "model.add(keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "model.add(keras.layers.Conv2D(64, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "model.add(keras.layers.MaxPool2D(pool_size=(2,2), padding='same'))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(13))\n",
        "model.add(keras.layers.Softmax())\n",
        "\n",
        "input_shape = train_images.shape\n",
        "model.build(input_shape)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# model.summary()\n",
        "\n",
        "print('>> Rule-biased model training')\n",
        "model.fit(train_images, train_mixed, batch_size=12, epochs=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-JrxVlruuu5",
        "outputId": "1263376c-8755-454f-8e58-7e05e98a12f0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Rule-biased model training\n",
            "Epoch 1/25\n",
            "667/667 [==============================] - 3s 4ms/step - loss: 0.3107 - accuracy: 0.9057\n",
            "Epoch 2/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.1057 - accuracy: 0.9705\n",
            "Epoch 3/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0714 - accuracy: 0.9806\n",
            "Epoch 4/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0533 - accuracy: 0.9851\n",
            "Epoch 5/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0397 - accuracy: 0.9874\n",
            "Epoch 6/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 0.0323 - accuracy: 0.9915\n",
            "Epoch 7/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0249 - accuracy: 0.9919\n",
            "Epoch 8/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0226 - accuracy: 0.9939\n",
            "Epoch 9/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 0.0184 - accuracy: 0.9951\n",
            "Epoch 10/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 0.0147 - accuracy: 0.9956\n",
            "Epoch 11/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0114 - accuracy: 0.9971\n",
            "Epoch 12/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0096 - accuracy: 0.9973\n",
            "Epoch 13/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 0.0060 - accuracy: 0.9979\n",
            "Epoch 14/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0057 - accuracy: 0.9985\n",
            "Epoch 15/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0067 - accuracy: 0.9981\n",
            "Epoch 16/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0032 - accuracy: 0.9991\n",
            "Epoch 17/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0022 - accuracy: 0.9990\n",
            "Epoch 18/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 5.7278e-04 - accuracy: 0.9996\n",
            "Epoch 19/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0017 - accuracy: 0.9995\n",
            "Epoch 20/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 6.3247e-04 - accuracy: 0.9996\n",
            "Epoch 21/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 0.0012 - accuracy: 0.9998\n",
            "Epoch 22/25\n",
            "667/667 [==============================] - 2s 4ms/step - loss: 0.0012 - accuracy: 0.9994\n",
            "Epoch 23/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 1.7482e-04 - accuracy: 1.0000\n",
            "Epoch 24/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 1.9807e-04 - accuracy: 1.0000\n",
            "Epoch 25/25\n",
            "667/667 [==============================] - 2s 3ms/step - loss: 7.0275e-04 - accuracy: 0.9999\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5d88374390>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## CONFUSIONS\n",
        "ims, labels = test_images, test_mixed\n",
        "labs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 'r', 'g', 'b']\n",
        "\n",
        "n = 20\n",
        "wrong_list = []\n",
        "test_ims = ims[:n]\n",
        "preds = model.predict(test_ims)\n",
        "for i in range(test_ims.shape[0]):\n",
        "  pred_max = max(preds[i]) # preds and ground are categorical, we must extract the number corresponding to the max value in the array.\n",
        "  ground_max = max(labels[i])\n",
        "  pred_value = list(preds[i]).index(pred_max)\n",
        "  ground_value = list(labels[i]).index(ground_max)\n",
        "\n",
        "  print('predicted value: ', labs[pred_value] , '  ground truth: ', labs[ground_value])\n",
        "\n",
        "  if pred_value != ground_value:\n",
        "    print(pred_value, ground_value)\n",
        "    wrong_list.append((i, pred_value))"
      ],
      "metadata": {
        "id": "StQsHOMDyw0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42742a9f-6c20-43be-c7e6-6f42e772bf46"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:5 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f5d0c29e4d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "predicted value:  r   ground truth:  r\n",
            "predicted value:  2   ground truth:  2\n",
            "predicted value:  r   ground truth:  r\n",
            "predicted value:  r   ground truth:  r\n",
            "predicted value:  4   ground truth:  4\n",
            "predicted value:  1   ground truth:  1\n",
            "predicted value:  4   ground truth:  4\n",
            "predicted value:  r   ground truth:  r\n",
            "predicted value:  5   ground truth:  5\n",
            "predicted value:  r   ground truth:  r\n",
            "predicted value:  r   ground truth:  r\n",
            "predicted value:  r   ground truth:  r\n",
            "predicted value:  9   ground truth:  9\n",
            "predicted value:  0   ground truth:  0\n",
            "predicted value:  r   ground truth:  r\n",
            "predicted value:  5   ground truth:  5\n",
            "predicted value:  9   ground truth:  9\n",
            "predicted value:  7   ground truth:  7\n",
            "predicted value:  3   ground truth:  3\n",
            "predicted value:  r   ground truth:  r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c, plots = 1, 1\n",
        "print('>> Wrongly classified : ', len(wrong_list), '/', test_ims.shape[0])\n",
        "plt.suptitle('prediction vs. label', y=0.7)\n",
        "labs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 'red', 'green', 'blue']\n",
        "for (i, j) in wrong_list:\n",
        "  ground_truth = list(test_numbers[i]) #is the ground truth categorical. How do I get the number ? \n",
        "  max_val = max(ground_truth)\n",
        "  true_label = ground_truth.index(max_val)\n",
        "  pred_label = labs[j]\n",
        "\n",
        "  if plots and wrong_list:\n",
        "    plt.subplot(1, len(wrong_list), c)\n",
        "    plt.title('{} / {}'.format(pred_label, true_label))\n",
        "    plt.imshow(test_ims[i])\n",
        "    plt.axis('off')\n",
        "  c += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "S9JKW8mq3mNx",
        "outputId": "ed6bb76e-9f24-4f4a-f18c-ae7f4b96ab7f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> Wrongly classified :  0 / 20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## POST-ANALYSIS\n",
        "if wrong_list:\n",
        "  rgb_confusion = np.zeros((3,))\n",
        "  for (i, j) in wrong_list: # i : index of wrongly classified image in the test dataset, # j : predicted label of that image.\n",
        "    im = test_ims[i]\n",
        "    for k in range(3):\n",
        "      if im[..., k].any():\n",
        "        rgb_confusion[k] += 1\n",
        "\n",
        "  rgb_confusion /= len(wrong_list)\n",
        "  pie_labels = ['red', 'green', 'blue']\n",
        "  plt.pie(rgb_confusion, labels = pie_labels, colors = pie_labels, autopct='%1.1f%%')\n",
        "  plt.title('is misclassification linked to color?')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "0zQrXt7_6tc-"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## REPRESENTATIONAL SIMILARITY ANALYSIS\n",
        "models = [color_model, shape_model, model]\n",
        "\n",
        "for model in models:\n",
        "  pass # use feature map analysis code developed last week"
      ],
      "metadata": {
        "id": "-XDrc5gXn6G5"
      },
      "execution_count": 56,
      "outputs": []
    }
  ]
}